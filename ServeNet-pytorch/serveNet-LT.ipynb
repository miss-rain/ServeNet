{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from Dataset import ServeNetDataset\n",
    "from data_pre import load_data_train, load_data_test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "UNCASED = './bert-base-uncased'\n",
    "VOCAB = 'vocab.txt'\n",
    "epochs = 40\n",
    "SEED = 123\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 0.01\n",
    "EPSILON = 1e-8\n",
    "BATCH_SIZE=128\n",
    "CLASS_NUM=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-hungarian",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evaluteTop1(model, dataLoader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(CLASS_NUM))\n",
    "    class_total = list(0. for i in range(CLASS_NUM))\n",
    "    with torch.no_grad():\n",
    "        for data in dataLoader:\n",
    "            input_tokens_name = data[3].cuda()\n",
    "            segment_ids_name = data[4].cuda()\n",
    "            input_masks_name = data[5].cuda()\n",
    "\n",
    "            input_tokens_descriptions = data[0].cuda()\n",
    "            segment_ids_descriptions = data[1].cuda()\n",
    "            input_masks_descriptions = data[2].cuda()\n",
    "            label = data[6].cuda()\n",
    "\n",
    "            outputs = model((input_tokens_name, segment_ids_name, input_masks_name),\n",
    "                            (input_tokens_descriptions, segment_ids_descriptions, input_masks_descriptions))\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "            #each class accuracy\n",
    "            c = (predicted == label).squeeze()\n",
    "            for i in range(len(label)):\n",
    "                labels = label[i]\n",
    "                class_correct[labels] += c[i].item()\n",
    "                class_total[labels] += 1\n",
    "\n",
    "\n",
    "    print('each class accuracy of: ' )\n",
    "    for i in range(CLASS_NUM):\n",
    "        #print('Accuracy of ======' ,100 * class_correct[i] / class_total[i])\n",
    "        print(100 * class_correct[i] / class_total[i])\n",
    "    \n",
    "    print('total class_total: ')\n",
    "    for i in range(CLASS_NUM):\n",
    "        print(class_total[i])\n",
    "        \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def evaluteTop5(model, dataLoader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataLoader:\n",
    "            input_tokens_name = data[3].cuda()\n",
    "            segment_ids_name = data[4].cuda()\n",
    "            input_masks_name = data[5].cuda()\n",
    "\n",
    "            input_tokens_descriptions = data[0].cuda()\n",
    "            segment_ids_descriptions = data[1].cuda()\n",
    "            input_masks_descriptions = data[2].cuda()\n",
    "            label = data[6].cuda()\n",
    "            outputs = model((input_tokens_name, segment_ids_name, input_masks_name),\n",
    "                            (input_tokens_descriptions, segment_ids_descriptions, input_masks_descriptions))\n",
    "            maxk = max((1, 5))\n",
    "            y_resize = label.view(-1, 1)\n",
    "            _, pred = outputs.topk(maxk, 1, True, True)\n",
    "            total += label.size(0)\n",
    "            correct += torch.eq(pred, y_resize).sum().float().item()\n",
    "   \n",
    "    return 100 * correct / total\n",
    "\n",
    "class weighted_sum(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(weighted_sum, self).__init__()\n",
    "        self.w1 = nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "        self.w2 = nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        return input1 * self.w1 + input2 * self.w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-penguin",
   "metadata": {
    "code_folding": [
     85
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class MutliHead(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 feat_dim=1024,\n",
    "                 num_classes=250,\n",
    "                 use_effect=True,\n",
    "                 num_head=2, #2, 4\n",
    "                 tau=16.0, #16, 32\n",
    "                 alpha=0, # 0, 1, 1.5, 3\n",
    "                 gamma=0.03125):\n",
    "        super(MutliHead, self).__init__()\n",
    "     \n",
    "        self.weight = nn.Parameter(torch.Tensor(num_classes, feat_dim), requires_grad=True)\n",
    "        self.scale = tau / num_head  \n",
    "        self.norm_scale = gamma       \n",
    "        self.alpha = alpha            \n",
    "        self.num_head = num_head\n",
    "        self.head_dim = feat_dim // num_head\n",
    "        self.use_effect = use_effect\n",
    "\n",
    "        self.MU = 1.0 - (1 - 0.9) * 0.02\n",
    "\n",
    "        self.causal_embed = nn.Parameter(torch.FloatTensor(1, feat_dim).fill_(1e-10), requires_grad=False)\n",
    "        \n",
    "        self.reset_parameters(self.weight)\n",
    "\n",
    "    def reset_parameters(self, weight):\n",
    "        nn.init.normal_(weight, 0, 0.01)\n",
    "\n",
    "    def get_cos_sin(self, x, y):\n",
    "        cos_val = (x * y).sum(-1, keepdim=True) / torch.norm(x, 2, 1, keepdim=True) / torch.norm(y, 2, 1, keepdim=True)\n",
    "        sin_val = (1 - cos_val * cos_val).sqrt()\n",
    "        return cos_val, sin_val\n",
    "\n",
    "    def multi_head_call(self, func, x, weight=None):\n",
    "        assert len(x.shape) == 2\n",
    "        x_list = torch.split(x, self.head_dim, dim=1)\n",
    "        if weight:\n",
    "            y_list = [func(item, weight) for item in x_list]\n",
    "        else:\n",
    "            y_list = [func(item) for item in x_list]\n",
    "        assert len(x_list) == self.num_head\n",
    "        assert len(y_list) == self.num_head\n",
    "        return torch.cat(y_list, dim=1)\n",
    "\n",
    "    def l2_norm(self, x):\n",
    "        normed_x = x / (torch.norm(x, 2, 1, keepdim=True) + 1e-8)\n",
    "        return normed_x\n",
    " \n",
    "    def causal_norm(self, x, weight):\n",
    "        norm= torch.norm(x, 2, 1, keepdim=True)\n",
    "        normed_x = x / (norm + weight)\n",
    "        return normed_x\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.reset_parameters(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normed_w = self.multi_head_call(self.causal_norm, self.weight, weight=self.norm_scale)\n",
    "        normed_x = self.multi_head_call(self.l2_norm, x)\n",
    "        y = torch.mm(normed_x * self.scale, normed_w.t())\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-grass",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ServeNet(torch.nn.Module):\n",
    "    def __init__(self, hiddenSize,CLASS_NUM):\n",
    "        super(ServeNet, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "\n",
    "        self.bert_name = BertModel.from_pretrained(UNCASED)\n",
    "        self.bert_description = BertModel.from_pretrained(UNCASED)\n",
    "\n",
    "        self.name_liner = nn.Linear(in_features=self.hiddenSize, out_features=1024)\n",
    "        self.name_ReLU = nn.ReLU()\n",
    "        self.name_Dropout = nn.Dropout(p=0.1)\n",
    " \n",
    "        self.lstm = nn.LSTM(input_size=self.hiddenSize, hidden_size=512, num_layers=1, batch_first=True,\n",
    "                            bidirectional=True)\n",
    "       \n",
    "        self.weight_sum = weighted_sum()\n",
    "        self.mutliHead = MutliHead()\n",
    "\n",
    "    def forward(self, names, descriptions):\n",
    "        self.lstm.flatten_parameters()\n",
    "        input_tokens_names, segment_ids_names, input_masks_names = names\n",
    "        input_tokens_descriptions, segment_ids_descriptions, input_masks_descriptions = descriptions\n",
    "\n",
    "        # name\n",
    "        name_bert_output = self.bert_name(input_tokens_names, segment_ids_names,\n",
    "                                     input_masks_names)\n",
    "        # Feature for Name\n",
    "        name_features = self.name_liner(name_bert_output[1])\n",
    "        name_features = self.name_ReLU(name_features)\n",
    "        name_features = self.name_Dropout(name_features)\n",
    "       \n",
    "        # description\n",
    "        description_bert_output = self.bert_description(input_tokens_descriptions, segment_ids_descriptions,\n",
    "                                                   input_masks_descriptions)\n",
    "\n",
    "        description_bert_feature=description_bert_output[0]\n",
    "\n",
    "        # LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(description_bert_feature)\n",
    "        hidden = torch.cat((cell[0, :, :], cell[1, :, :]), dim=1)\n",
    "        #hidden = torch.cat((hidden[0, :, :], hidden[1, :, :]), dim=1)\n",
    "\n",
    "        # sum\n",
    "        all_features = self.weight_sum(name_features, hidden)\n",
    "        output = self.mutliHead(all_features)\n",
    "       \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-network",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_data = load_data_train(CLASS_NUM)\n",
    "    test_data = load_data_test(CLASS_NUM)\n",
    "    # train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = ServeNet(768,CLASS_NUM)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "    # optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:{},lr:{}\".format(str(epoch+1),str(optimizer.state_dict()['param_groups'][0]['lr'])))\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        for data in tqdm(train_dataloader):\n",
    "\n",
    "            input_tokens_name = data[3].cuda()\n",
    "            segment_ids_name = data[4].cuda()\n",
    "            input_masks_name = data[5].cuda()\n",
    "\n",
    "            input_tokens_descriptions = data[0].cuda()\n",
    "            segment_ids_descriptions = data[1].cuda()\n",
    "            input_masks_descriptions = data[2].cuda()\n",
    "            label = data[6].cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model((input_tokens_name, segment_ids_name, input_masks_name),\n",
    "                            (input_tokens_descriptions, segment_ids_descriptions, input_masks_descriptions))\n",
    "\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"=======>top1 acc on the test:{}\".format(str(evaluteTop1(model, test_dataloader))))\n",
    "        print(\"=======>top5 acc on the test:{}\".format(str(evaluteTop5(model, test_dataloader))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
